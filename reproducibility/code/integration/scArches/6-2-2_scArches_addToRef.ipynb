{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** scArches integration of new datasets to reference \n",
    "# (integration run selected from the original test on the smaller ref data subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scarches as sca\n",
    "import datetime\n",
    "import os\n",
    "import argparse\n",
    "import glob\n",
    "import anndata as ann\n",
    "import numpy as np\n",
    "\n",
    "import sys  \n",
    "sys.path.insert(0, '/lustre/groups/ml01/workspace/karin.hrovatin/code/diabetes_analysis/')\n",
    "import helper as h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for loading data and saving results\n",
    "path_ref='/lustre/groups/ml01/workspace/karin.hrovatin/data/pancreas/scRNA/ref_combined/preprocessed/'\n",
    "path_model='/lustre/groups/ml01/workspace/karin.hrovatin/data/pancreas/scRNA/ref_combined/scArches/'\n",
    "path_out='/lustre/groups/ml01/workspace/karin.hrovatin/data/pancreas/scRNA/combined/scArches/integrate_add_ref/'\n",
    "#Unique ID2 for reading/writing h5ad files with helper function\n",
    "UID2='scArches_addToRef'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the initially tested scArches models with different parameters on the ref subset we manually copied the selected one into a directory called ref_run, as the directories themselvles are named based on timestamp, so they would never have comparable name across re-runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old params: {'z_dimension': 15, 'architecture': [128, 128, 128], 'task_name': 'run_scArches1601891506.923574', 'x_dimension': 2000, 'beta': 0.0, 'alpha': 0.99, 'loss_fn': 'sse', 'n_epochs': 150, 'batch_size': 128, 'subset_beta': False, 'hvg_n': '2000'}\n",
      "Corrected old params: {'z_dimension': 15, 'architecture': [128, 128, 128], 'task_name': 'ref_run', 'x_dimension': 2000, 'beta': 0.0, 'alpha': 0.99, 'loss_fn': 'sse', 'n_epochs': 150, 'batch_size': 128, 'subset_beta': False, 'hvg_n': '2000'}\n"
     ]
    }
   ],
   "source": [
    "# Load parameters of ref model\n",
    "task_name_ref='ref_run'\n",
    "params=pickle.load(open(path_model+task_name_ref+'/params.pkl','rb'))\n",
    "print('Old params:',params)\n",
    "params['task_name']=task_name_ref\n",
    "print('Corrected old params:',params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New params {'z_dimension': 15, 'architecture': [128, 128, 128], 'task_name': 'run_scArches1603270547.313737', 'x_dimension': 2000, 'beta': 0.0, 'alpha': 0.99, 'loss_fn': 'sse', 'n_epochs': 150, 'batch_size': 128, 'subset_beta': False, 'hvg_n': '2000', 'sca_version': 'scArches v2'}\n"
     ]
    }
   ],
   "source": [
    "# Change params for new networks\n",
    "params_new=params.copy()\n",
    "params_new['task_name']='run_scArches'+str(datetime.datetime.now().timestamp())\n",
    "params_new['sca_version']='scArches v2'\n",
    "#params_new['early_stop']=early_stop\n",
    "print('New params',params_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Reference adata\n",
    "adata_ref=h.open_h5ad(file=path_ref+'data_normalised.h5ad',unique_id2=UID2)\n",
    "\n",
    "adata_ref.var.rename(columns={'highly_variable': 'highly_variable_2000'}, inplace=True)\n",
    "hvg_col='highly_variable_'+params['hvg_n']\n",
    "# Select HVG\n",
    "adata_ref=adata_ref[:,adata_ref.var[hvg_col]]\n",
    "\n",
    "# Ensure that adata.raw.X (used by scArches in nb) has the same genes as adata.X\n",
    "adata_ref.raw=adata_ref.raw[:,adata_ref.raw.var_names.isin(adata_ref.var_names)].to_adata()\n",
    "# Rename size factors so that scArches finds them\n",
    "if 'size_factors' in adata_ref.obs.columns:\n",
    "    print('Size factors are already present - renaming them to size_factors_old')\n",
    "adata_ref.obs.rename(columns={'size_factors': 'size_factors_old', 'size_factors_sample': 'size_factors'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOD\n",
      "Preprocessed data shape: (2690, 15034)\n",
      "Integration data shape: (2690, 2000)\n",
      "NOD_elimination\n",
      "Preprocessed data shape: (54329, 18696)\n",
      "Integration data shape: (54329, 2000)\n",
      "spikein_drug\n",
      "Preprocessed data shape: (33331, 17709)\n",
      "Integration data shape: (33331, 2000)\n",
      "embryo\n",
      "Preprocessed data shape: (37561, 17631)\n",
      "Integration data shape: (37561, 2000)\n",
      "VSG\n",
      "Preprocessed data shape: (57157, 19958)\n",
      "Integration data shape: (57157, 2000)\n",
      "STZ\n",
      "Preprocessed data shape: (41150, 17846)\n",
      "Integration data shape: (41150, 2000)\n"
     ]
    }
   ],
   "source": [
    "# *** New data\n",
    "data=[('NOD','/lustre/groups/ml01/workspace/karin.hrovatin/data/pancreas/scRNA/GSE144471/data_normlisedForIntegration.h5ad'),\n",
    "      ('NOD_elimination','/lustre/groups/ml01/workspace/karin.hrovatin/data/pancreas/scRNA/GSE117770/data_normlisedForIntegration.h5ad'),\n",
    "      ('spikein_drug','/lustre/groups/ml01/workspace/karin.hrovatin/data/pancreas/scRNA/GSE142465/data_normlisedForIntegration.h5ad'),\n",
    "      ('embryo','/lustre/groups/ml01/workspace/karin.hrovatin/data/pancreas/scRNA/GSE132188/rev7/data_normlisedForIntegration.h5ad'),\n",
    "      ('VSG','/lustre/groups/ml01/workspace/karin.hrovatin/data/pancreas/scRNA/VSG_PF_WT_cohort/rev7/nonref/data_normlisedForIntegration.h5ad'),\n",
    "      ('STZ','/lustre/groups/ml01/workspace/karin.hrovatin/data/pancreas/scRNA/islet_glpest_lickert/rev7/nonref/data_normlisedForIntegration.h5ad')]\n",
    "\n",
    "adatas=[]\n",
    "for study,path_pp in data:\n",
    "    print(study)\n",
    "    #Load data\n",
    "    adata_pp=h.open_h5ad(file=path_pp,unique_id2=UID2)\n",
    "    print('Preprocessed data shape:',adata_pp.shape)  \n",
    "    # Extract raw data, subset it to QC cells and integration genes and normalise raw\n",
    "    adata=adata_pp.raw.to_adata()\n",
    "    adata=adata[adata_pp.obs_names,adata_ref.var_names]\n",
    "    adata.obs=adata_pp.obs.copy()\n",
    "    # Normalise raw data with previously compited size factors\n",
    "    adata.raw=adata.copy()\n",
    "    adata.X /= adata.obs['size_factors_sample'].values[:,None] # This reshapes the size-factors array\n",
    "    sc.pp.log1p(adata)\n",
    "    adata.X = np.asarray(adata.X)\n",
    "    adatas.append(adata)\n",
    "    print('Integration data shape:',adata.shape) \n",
    "\n",
    "# Combine datasets    \n",
    "adata_query = ann.AnnData.concatenate( *adatas,  batch_key = 'study', batch_categories = [d[0] for d in data ]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename size factors so that scArches finds them\n",
    "if 'size_factors' in adata_query.obs.columns:\n",
    "    print('Size factors are already present - renaming them to size_factors_old')\n",
    "else:\n",
    "    adata_query.obs.rename(columns={'size_factors': 'size_factors_old', 'size_factors_sample': 'size_factors'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference adata\n",
      "AnnData object with n_obs × n_vars = 73487 × 2000\n",
      "    obs: 'file', 'n_counts', 'n_genes', 'mt_frac', 'doublet_score', 'size_factors_study', 'pre_cell_type', 'S_score', 'G2M_score', 'phase', 'phase_cyclone', 's_cyclone', 'g2m_cyclone', 'g1_cyclone', 'x_score', 'y_score', 'sex', 'age', 'design', 'strain', 'tissue', 'technique', 'study', 'study_sample', 'size_factors', 'cell_type', 'cell_type_multiplet', 'cell_subtype', 'cell_subtype_multiplet', 'true_type'\n",
      "    var: 'n_cells-Fltp_2y', 'n_cells-Fltp_P16', 'n_cells-Fltp_adult', 'n_cells-STZ', 'n_cells-VSG', 'highly_variable_2000', 'highly_variable_5000', 'highly_variable_2000_beta', 'highly_variable_2000_Fltp_P16', 'highly_variable_2000_beta_Fltp_P16', 'highly_variable_2000_Fltp_adult', 'highly_variable_2000_Fltp_2y', 'highly_variable_2000_VSG', 'highly_variable_2000_STZ', 'highly_variable_2000_beta_Fltp_adult', 'highly_variable_2000_beta_Fltp_2y', 'highly_variable_2000_beta_VSG', 'highly_variable_2000_beta_STZ', 'highly_variable_samples_target2000', 'highly_variable_samples_target2000_beta', 'highly_variable_multilevel_target2000'\n",
      "    uns: 'age_colors', 'cell_type_colors', 'file_colors', 'neighbors', 'pca', 'phase_cyclone_colors', 'sex_colors', 'study_colors', 'study_sample_colors', 'tissue_colors', 'umap'\n",
      "    obsm: 'X_pca', 'X_umap'\n",
      "    varm: 'PCs'\n",
      "    layers: 'counts'\n",
      "    obsp: 'connectivities', 'distances'\n",
      "Query adata\n",
      "AnnData object with n_obs × n_vars = 226218 × 2000\n",
      "    obs: 'file', 'study', 'study_sample', 'reference', 'size_factors'\n"
     ]
    }
   ],
   "source": [
    "print('Reference adata')\n",
    "print(adata_ref)\n",
    "print('Query adata')\n",
    "print(adata_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/icb/karin.hrovatin/miniconda3/envs/py3.6-scarches/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/icb/karin.hrovatin/miniconda3/envs/py3.6-scarches/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/icb/karin.hrovatin/miniconda3/envs/py3.6-scarches/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/icb/karin.hrovatin/miniconda3/envs/py3.6-scarches/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/icb/karin.hrovatin/miniconda3/envs/py3.6-scarches/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/icb/karin.hrovatin/miniconda3/envs/py3.6-scarches/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/icb/karin.hrovatin/miniconda3/envs/py3.6-scarches/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/icb/karin.hrovatin/miniconda3/envs/py3.6-scarches/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/icb/karin.hrovatin/miniconda3/envs/py3.6-scarches/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/icb/karin.hrovatin/miniconda3/envs/py3.6-scarches/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "scArches' network has been successfully constructed!\n",
      "WARNING:tensorflow:From /home/icb/karin.hrovatin/miniconda3/envs/py3.6-scarches/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/icb/karin.hrovatin/miniconda3/envs/py3.6-scarches/lib/python3.6/site-packages/scarches/scarches/models/_losses.py:46: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/icb/karin.hrovatin/miniconda3/envs/py3.6-scarches/lib/python3.6/site-packages/scarches/scarches/models/_losses.py:46: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/icb/karin.hrovatin/miniconda3/envs/py3.6-scarches/lib/python3.6/site-packages/scarches/scarches/models/_utils.py:84: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/icb/karin.hrovatin/miniconda3/envs/py3.6-scarches/lib/python3.6/site-packages/scarches/scarches/models/_utils.py:84: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "scArches' network has been successfully compiled!\n"
     ]
    }
   ],
   "source": [
    "# *** Restore model\n",
    "network = sca.models.scArches(task_name=params['task_name'],\n",
    "    x_dimension=params['x_dimension'],\n",
    "    z_dimension=params['z_dimension'],\n",
    "    architecture=params['architecture'],\n",
    "    gene_names=adata_ref.var_names.tolist(),\n",
    "    conditions=adata_ref.obs['study_sample'].unique().tolist(),\n",
    "    alpha=params['alpha'], \n",
    "    beta=params['beta'],\n",
    "    loss_fn=params['loss_fn'],\n",
    "    model_path=path_model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/icb/karin.hrovatin/miniconda3/envs/py3.6-scarches/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/icb/karin.hrovatin/miniconda3/envs/py3.6-scarches/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "scArches' network has been successfully compiled!\n",
      "cvae's weights has been successfully restored!\n"
     ]
    }
   ],
   "source": [
    "network.train(adata_ref,\n",
    "              condition_key='study_sample',\n",
    "              retrain=False,\n",
    "              # These conditions are not really needed as network is not retrained\n",
    "              n_epochs=params['n_epochs'],\n",
    "              batch_size=params['batch_size'],\n",
    "              save=False\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scArches' network has been successfully constructed!\n",
      "scArches' network has been successfully compiled!\n",
      "WARNING:tensorflow:From /home/icb/karin.hrovatin/miniconda3/envs/py3.6-scarches/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/icb/karin.hrovatin/miniconda3/envs/py3.6-scarches/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      " |████████████████████| 100.0%  - loss: 373.6227 - mmd_loss: 0.0000 - recon_loss: 373.6227 - val_loss: 94.3628 - val_mmd_loss: 0.0000 - val_recon_loss: 94.3628\n",
      "\n",
      "scArches has been successfully saved in /lustre/groups/ml01/workspace/karin.hrovatin/data/pancreas/scRNA/combined/scArches/integrate_add_ref/run_scArches1603270547.313737/.\n"
     ]
    }
   ],
   "source": [
    "# *** Modify scArches model with new studies\n",
    "network_new=network\n",
    "\n",
    "network_new = sca.operate(network_new,\n",
    "    new_task_name=params_new['task_name'],\n",
    "    new_conditions=adata_query.obs['study_sample'].unique(),\n",
    "    # Does not work, so change below\n",
    "    #new_network_kwargs={'model_path':path_out}\n",
    "    # Mo's suggestion\n",
    "    #new_network_kwargs={'use_batchnorm':params_new['use_batchnorm']},\n",
    "    version=params_new['sca_version']\n",
    "                     )\n",
    "network_new.model_path=path_out+network_new.task_name+os.sep\n",
    "network_new.train(adata_query,\n",
    "          condition_key='study_sample',\n",
    "          retrain=True,\n",
    "          n_epochs=params_new['n_epochs'],\n",
    "          batch_size=params_new['batch_size'],\n",
    "          save=True,\n",
    "          #early_stop_limit=params_new['early_stop']\n",
    "         )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Save integration results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save params\n",
    "pickle.dump(params_new, open( path_out+params_new['task_name']+\"/params.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent adata:\n",
      " (299705, 15)\n"
     ]
    }
   ],
   "source": [
    "# Get latent representation\n",
    "# Combined data for latent representation\n",
    "adata = ann.AnnData.concatenate( adata_query,adata_ref, batch_categories =['nonref','ref']).copy()\n",
    "# Make sure that obs_names match ref\n",
    "adata.obs_names=[name.replace('-ref','').replace('-nonref','') for name in adata.obs_names]\n",
    "latent_adata = network_new.get_latent(adata, 'study_sample')\n",
    "print('Latent adata:\\n',latent_adata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute neighbours and UMAP\n",
    "sc.pp.neighbors(latent_adata,n_pcs=0)\n",
    "sc.tl.umap(latent_adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "... storing 'file' as categorical\n",
      "... storing 'study' as categorical\n",
      "... storing 'study_sample' as categorical\n",
      "... storing 'reference' as categorical\n",
      "... storing 'pre_cell_type' as categorical\n",
      "... storing 'phase' as categorical\n",
      "... storing 'phase_cyclone' as categorical\n",
      "... storing 'sex' as categorical\n",
      "... storing 'age' as categorical\n",
      "... storing 'design' as categorical\n",
      "... storing 'strain' as categorical\n",
      "... storing 'tissue' as categorical\n",
      "... storing 'technique' as categorical\n",
      "... storing 'cell_type' as categorical\n",
      "... storing 'cell_type_multiplet' as categorical\n",
      "... storing 'cell_subtype' as categorical\n",
      "... storing 'cell_subtype_multiplet' as categorical\n",
      "... storing 'true_type' as categorical\n"
     ]
    }
   ],
   "source": [
    "# Save latent data\n",
    "h.save_h5ad(adata=latent_adata,file=path_out+params_new['task_name']+'/latent.h5ad',unique_id2=UID2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "py3.6-scarches",
   "language": "python",
   "name": "py3.6-scarches"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
