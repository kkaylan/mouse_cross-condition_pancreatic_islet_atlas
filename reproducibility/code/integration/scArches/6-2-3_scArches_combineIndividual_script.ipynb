{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scArches integration of multiple datasets from different adatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/icb/karin.hrovatin/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/icb/karin.hrovatin/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/icb/karin.hrovatin/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/icb/karin.hrovatin/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/icb/karin.hrovatin/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/icb/karin.hrovatin/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/icb/karin.hrovatin/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/icb/karin.hrovatin/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/icb/karin.hrovatin/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/icb/karin.hrovatin/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/icb/karin.hrovatin/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/icb/karin.hrovatin/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/icb/karin.hrovatin/miniconda3/envs/py3.6-scarches/lib/python3.6/site-packages/scarches/scarches/models/cvae.py:15: The name tf.random.set_random_seed is deprecated. Please use tf.compat.v1.random.set_random_seed instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scarches as sca\n",
    "import datetime\n",
    "import os\n",
    "import argparse\n",
    "import anndata as ann\n",
    "import scIB as scib\n",
    "\n",
    "sys.path.insert(0, '/lustre/groups/ml01/code/karin.hrovatin/diabetes_analysis/')\n",
    "import helper as h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths for loading ref params\n",
    "path_refmodel='/lustre/groups/ml01/workspace/karin.hrovatin/data/pancreas/scRNA/ref_combined/scArches/ref_run/'\n",
    "# Path for storing results\n",
    "path_out='/lustre/groups/ml01/workspace/karin.hrovatin/data/pancreas/scRNA/combined/scArches/integrate_combine_individual/'\n",
    "\n",
    "data=[('Fltp_2y','/lustre/groups/ml01/workspace/karin.hrovatin/data/pancreas/scRNA/islets_aged_fltp_iCre/rev6/'),\n",
    "      ('Fltp_adult','/lustre/groups/ml01/workspace/karin.hrovatin/data/pancreas/scRNA/islet_fltp_headtail/rev4/'),\n",
    "      ('Fltp_P16','/lustre/groups/ml01/workspace/karin.hrovatin/data/pancreas/scRNA/salinno_project/rev4/'),\n",
    "      ('NOD','/lustre/groups/ml01/workspace/karin.hrovatin/data/pancreas/scRNA/GSE144471/'),\n",
    "      ('NOD_elimination','/lustre/groups/ml01/workspace/karin.hrovatin/data/pancreas/scRNA/GSE117770/'),\n",
    "      ('spikein_drug','/lustre/groups/ml01/workspace/karin.hrovatin/data/pancreas/scRNA/GSE142465/'),\n",
    "      ('embryo','/lustre/groups/ml01/workspace/karin.hrovatin/data/pancreas/scRNA/GSE132188/rev7/'),\n",
    "      ('VSG','/lustre/groups/ml01/workspace/karin.hrovatin/data/pancreas/scRNA/VSG_PF_WT_cohort/rev7/'),\n",
    "      ('STZ','/lustre/groups/ml01/workspace/karin.hrovatin/data/pancreas/scRNA/islet_glpest_lickert/rev7/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # For testing\n",
    "    UID3='a'\n",
    "    input_file_name='data_normlisedForIntegration.h5ad'\n",
    "    path_subset='/lustre/groups/ml01/workspace/karin.hrovatin/data/pancreas/scRNA/combined/beta_cell_names.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sys.argv)\n",
    "UID3=sys.argv[1]\n",
    "input_file_name=sys.argv[2]\n",
    "# See default above\n",
    "if len(sys.argv)>3:\n",
    "    arg3=sys.argv[3]\n",
    "    if arg3!=\"None\":\n",
    "        data=[tuple(study.split(',')) for study in arg3.split(';') ]\n",
    "        print('Using data:\\n',data)   \n",
    "# See default above\n",
    "if len(sys.argv)>4:\n",
    "    arg4=sys.argv[4]\n",
    "    if arg4!='None':\n",
    "        path_out=arg4\n",
    "        print('Will save to:',path_out)\n",
    "path_subset=None\n",
    "if len(sys.argv)>5:\n",
    "    arg5=sys.argv[5]\n",
    "    if arg5!='None':\n",
    "        path_subset=arg5\n",
    "        print('Will subset cells based on:',path_subset)\n",
    "alpha=None\n",
    "if len(sys.argv)>6:\n",
    "    arg6=sys.argv[6]\n",
    "    if arg6!='None':\n",
    "        alpha=float(arg6)\n",
    "        print('Reset alpha to:',alpha)\n",
    "loss=None\n",
    "if len(sys.argv)>7:\n",
    "    arg7=sys.argv[7]\n",
    "    if arg7!='None':\n",
    "        loss=arg7\n",
    "        print('Reset loss to:',loss)\n",
    "quick=False\n",
    "if len(sys.argv)>8:\n",
    "    # 0 (False) or 1  (True) or None for default (False)\n",
    "    arg8=sys.argv[8]\n",
    "    if arg8!='None':\n",
    "        quick=bool(int(arg8))\n",
    "        print('Stopping is quick:',quick)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unique ID2 for reading/writing h5ad files with helper function\n",
    "UID2='scArches_combineIndividual'+UID3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data and params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old params: {'z_dimension': 15, 'architecture': [128, 128, 128], 'task_name': 'run_scArches1601891506.923574', 'x_dimension': 2000, 'beta': 0.0, 'alpha': 0.99, 'loss_fn': 'sse', 'n_epochs': 150, 'batch_size': 128, 'subset_beta': False, 'hvg_n': '2000'}\n"
     ]
    }
   ],
   "source": [
    "# Load parameters of ref model\n",
    "params=pickle.load(open(path_refmodel+'params.pkl','rb'))\n",
    "print('Old params:',params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New params {'z_dimension': 15, 'architecture': [128, 128, 128], 'task_name': 'run_scArches1612794665.125492', 'x_dimension': 2000, 'beta': 0.0, 'alpha': 0.99, 'loss_fn': 'sse', 'n_epochs': 150, 'batch_size': 128, 'subset_beta': False, 'hvg_n': '2000', 'input_file_name': 'data_normlisedForIntegration.h5ad'}\n"
     ]
    }
   ],
   "source": [
    "# Change params for new networks\n",
    "params_new=params.copy()\n",
    "params_new['task_name']='run_scArches'+str(datetime.datetime.now().timestamp())\n",
    "params_new['input_file_name']=input_file_name\n",
    "if not quick:\n",
    "    params_new['learning_rate']=0.0001\n",
    "    params_new['early_stop_limit']=30\n",
    "\n",
    "if alpha is not None:\n",
    "    params_new['alpha']=alpha\n",
    "if loss is not None:\n",
    "    params_new['loss_fn']=loss\n",
    "    \n",
    "print('New params',params_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fltp_2y\n",
      "(17361, 17146)\n",
      "Fltp_adult\n",
      "(17353, 16430)\n",
      "Fltp_P16\n",
      "(19881, 16773)\n",
      "NOD\n",
      "(2690, 15034)\n",
      "NOD_elimination\n",
      "(54329, 18696)\n",
      "spikein_drug\n",
      "(33331, 17709)\n",
      "embryo\n",
      "(37561, 17631)\n",
      "VSG\n",
      "(69745, 20130)\n",
      "STZ\n",
      "(49545, 18004)\n"
     ]
    }
   ],
   "source": [
    "# *** New data\n",
    "adatas=[]\n",
    "for study,path in data:\n",
    "    print(study)\n",
    "    #Load data\n",
    "    adata=h.open_h5ad(file=path+input_file_name,unique_id2=UID2)\n",
    "    print(adata.shape)            \n",
    "    adatas.append(adata)\n",
    "    \n",
    "# Combine datasets    \n",
    "adata = ann.AnnData.concatenate( *adatas,  batch_key = 'study', \n",
    "                                batch_categories = [d[0] for d in data ]).copy()\n",
    "# Edit obs_names to match reference\n",
    "adata.obs_names=[name.replace('_ref','').replace('_nonref','') for name in adata.obs_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting cells. N cells to subset: 103993 N present cells to subset: 103993\n",
      "N cells before subset: 104030\n",
      "N cells after subset: 103993\n"
     ]
    }
   ],
   "source": [
    "# Subset cells \n",
    "if path_subset is not None:\n",
    "    cells_sub=pickle.load(open(path_subset,'rb'))\n",
    "    cells_sub_present=[cell for cell in cells_sub if cell in adata.obs_names]\n",
    "    print('Subsetting cells. N cells to subset:',len(cells_sub),\n",
    "          'N present cells to subset:',len(cells_sub_present))\n",
    "    print('N cells before subset:',adata.shape[0])\n",
    "    adata=adata[cells_sub_present,:]\n",
    "    print('N cells after subset:',adata.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size factors are already present - renaming them to size_factors_old\n"
     ]
    }
   ],
   "source": [
    "# Rename size factors so that scArches finds them\n",
    "if 'size_factors' in adata.obs.columns:\n",
    "    print('Size factors are already present - renaming them to size_factors_old')\n",
    "adata.obs.rename(columns={'size_factors': 'size_factors_old', 'size_factors_sample': 'size_factors'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to set attribute `.obs` of view, copying.\n",
      "... storing 'file' as categorical\n",
      "... storing 'reference' as categorical\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 14 HVGs from full intersect set\n",
      "Using 10 HVGs from n_batch-1 set\n",
      "Using 16 HVGs from n_batch-2 set\n",
      "Using 12 HVGs from n_batch-3 set\n",
      "Using 9 HVGs from n_batch-4 set\n",
      "Using 11 HVGs from n_batch-5 set\n",
      "Using 17 HVGs from n_batch-6 set\n",
      "Using 15 HVGs from n_batch-7 set\n",
      "Using 16 HVGs from n_batch-8 set\n",
      "Using 13 HVGs from n_batch-9 set\n",
      "Using 13 HVGs from n_batch-10 set\n",
      "Using 20 HVGs from n_batch-11 set\n",
      "Using 15 HVGs from n_batch-12 set\n",
      "Using 18 HVGs from n_batch-13 set\n",
      "Using 22 HVGs from n_batch-14 set\n",
      "Using 23 HVGs from n_batch-15 set\n",
      "Using 33 HVGs from n_batch-16 set\n",
      "Using 18 HVGs from n_batch-17 set\n",
      "Using 29 HVGs from n_batch-18 set\n",
      "Using 34 HVGs from n_batch-19 set\n",
      "Using 40 HVGs from n_batch-20 set\n",
      "Using 36 HVGs from n_batch-21 set\n",
      "Using 33 HVGs from n_batch-22 set\n",
      "Using 36 HVGs from n_batch-23 set\n",
      "Using 43 HVGs from n_batch-24 set\n",
      "Using 50 HVGs from n_batch-25 set\n",
      "Using 47 HVGs from n_batch-26 set\n",
      "Using 47 HVGs from n_batch-27 set\n",
      "Using 63 HVGs from n_batch-28 set\n",
      "Using 64 HVGs from n_batch-29 set\n",
      "Using 93 HVGs from n_batch-30 set\n",
      "Using 100 HVGs from n_batch-31 set\n",
      "Using 96 HVGs from n_batch-32 set\n",
      "Using 103 HVGs from n_batch-33 set\n",
      "Using 119 HVGs from n_batch-34 set\n",
      "Using 119 HVGs from n_batch-35 set\n",
      "Using 153 HVGs from n_batch-36 set\n",
      "Using 187 HVGs from n_batch-37 set\n",
      "Using 213 HVGs from n_batch-38 set\n",
      "Using 2000 HVGs\n",
      "Number of highly variable genes: 2000\n"
     ]
    }
   ],
   "source": [
    "# Compute HVGs on combined dataset\n",
    "if params_new['hvg_n']=='2000':\n",
    "    # Compute HVG across batches (samples) using scIB function\n",
    "    adata.obs['study_sample'] = adata.obs['study_sample'].astype('category')\n",
    "    hvgs=scib.preprocessing.hvg_batch(adata, batch_key='study_sample', target_genes=2000, flavor='cell_ranger')\n",
    "    # Add HVGs to adata\n",
    "    hvgs=pd.DataFrame([True]*len(hvgs),index=hvgs,columns=['highly_variable'])\n",
    "    hvgs=hvgs.reindex(adata.var_names,copy=False)\n",
    "    hvgs=hvgs.fillna(False)\n",
    "    adata.var['highly_variable']=hvgs.values\n",
    "    print('Number of highly variable genes: {:d}'.format(adata.var['highly_variable'].sum()))\n",
    "else:\n",
    "    raise ValueError('HVG mode in params not recongnised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adata shape: (103993, 2000) Raw shape: (103993, 2000)\n"
     ]
    }
   ],
   "source": [
    "# Retain only HVGs\n",
    "adata=adata[:,adata.var['highly_variable']]\n",
    "# Ensure that adata.raw.X (used by scArches in nb) has the same genes as adata.X\n",
    "adata.raw=adata.raw[:,adata.raw.var_names.isin(adata.var_names)].to_adata()\n",
    "print('Adata shape:',adata.shape,'Raw shape:',adata.raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "if not quick:\n",
    "    network = sca.models.scArches(task_name=params_new['task_name'],\n",
    "        x_dimension=params_new['x_dimension'],\n",
    "        z_dimension=params_new['z_dimension'],\n",
    "        architecture=params_new['architecture'],\n",
    "        gene_names=adata.var_names.tolist(),\n",
    "        conditions=adata.obs['study_sample'].unique().tolist(),\n",
    "        alpha=params_new['alpha'], \n",
    "        beta=params_new['beta'],\n",
    "        loss_fn=params_new['loss_fn'],\n",
    "        model_path=path_out,\n",
    "        learning_rate=params_new['learning_rate']\n",
    "        )\n",
    "else:\n",
    "    network = sca.models.scArches(task_name=params_new['task_name'],\n",
    "        x_dimension=params_new['x_dimension'],\n",
    "        z_dimension=params_new['z_dimension'],\n",
    "        architecture=params_new['architecture'],\n",
    "        gene_names=adata.var_names.tolist(),\n",
    "        conditions=adata.obs['study_sample'].unique().tolist(),\n",
    "        alpha=params_new['alpha'], \n",
    "        beta=params_new['beta'],\n",
    "        loss_fn=params_new['loss_fn'],\n",
    "        model_path=path_out,\n",
    "        )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run scArches\n",
    "if not quick:\n",
    "    network.train(adata,\n",
    "              n_epochs=params_new['n_epochs'],\n",
    "              batch_size=params_new['batch_size'], \n",
    "              condition_key='study_sample',\n",
    "              save=True,\n",
    "              retrain=True,\n",
    "              early_stop_limit=params_new['early_stop_limit']\n",
    "             )\n",
    "else:\n",
    "    network.train(adata,\n",
    "              n_epochs=params_new['n_epochs'],\n",
    "              batch_size=params_new['batch_size'], \n",
    "              condition_key='study_sample',\n",
    "              save=True,\n",
    "              retrain=True,\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save additional information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save=path_out+params_new['task_name']+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save params\n",
    "pickle.dump(params_new, open( path_save+\"params.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get latent reepresentation\n",
    "latent_adata = network.get_latent(adata, 'study_sample')\n",
    "latent_adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute neighbours and UMAP\n",
    "sc.pp.neighbors(latent_adata,n_pcs=0)\n",
    "sc.tl.umap(latent_adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save latent data\n",
    "h.save_h5ad(adata=latent_adata,file=path_save+'/latent.h5ad',unique_id2=UID2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "py3.6-scarches",
   "language": "python",
   "name": "py3.6-scarches"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
